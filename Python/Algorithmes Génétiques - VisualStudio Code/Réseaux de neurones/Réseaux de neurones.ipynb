{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les réseaux de neurones artificiels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux de neurones artificiels sont issus de la recherche scientifique.\n",
    "S'inspirant fortement du cerveau, ils peuvent sembler complexes au premier abord. Ce tutoriel vise à démystifier un modèle d'apprentissage qui n'est finalement pas aussi terrifiant que cela. (Si l'on passe outre la partie mathématique, qui, elle, peut effrayer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer, nous allons importer LE module qui nous sera utile : numpy.\n",
    "Ce module surcouche énormément de méthodes de calcul, rendant leur utilisation triviale. Numpy propose un ensemble d'optimisations de calcul et fait même appel à du langage C pour accélerer encore ses traitements. En résumé, Numpy est simple et fait gagner du temps. Ce module est utilisé dans la majeure partie des autres modules touchant un minimum aux mathématiques, le rendant essentiel dans la plupart des cas. Par convention, nous lui affectons un alias : \"np\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce tutoriel est assez simple. Nous cherchons à apprendre à la machine la porte logique XOR (OU EXCLUSIF). Pour rappel, la table de vérité du XOR est la suivante:\n",
    "\n",
    "| X1 | X2 |--| Y |\n",
    "|----|----|--|---|\n",
    "|  0 |  0 |--| 0 |\n",
    "|  0 |  1 |--| 1 |\n",
    "|  1 |  0 |--| 1 |\n",
    "|  1 |  1 |--| 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables X et y recoivent donc l'ensemble des valeurs de la table de vérité.\n",
    "X représente maintenant les entrées de notre jeu d'entrainement. y représente les sorties attendues pour notre réseau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous concevons maintenant notre réseau.\n",
    "Ce réseau prendra une couche cachée d'une taille choisie (le paramètre \"taille\" à modifier). Plus la taille de la couche cachée est grande, plus le traitement devient long. Dans le cas général, il s'agit d'une valeur à choisir avec une relative précision. Dans ce cas précis, l'influence est faible.\n",
    "Les poids sont alors initialisés de manière aléatoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer, les poids placés entre la couche d'entrée (de taille 2 car chaque X est composé de 2 valeurs) et la couche cachée (d'une taille à définir), sont initialisés entre -1 et 1 (syn0).\n",
    "Il en va de même pour les poids situés entre la couche cachée (toujours de taille à définir) et la couche de sortie (de taille 1 car seule une valeur est attendue) (syn1).\n",
    "Un compteur d'entrainement est initialisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taille = ???\n",
    "syn0 = 2*np.random.random((2,taille)) - 1\n",
    "syn1 = 2*np.random.random((taille,1)) - 1\n",
    "compteur = 0\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vient ensuite la boucle d'entrainement (le nombre de passages dans la boucle est à définir en modifiant \"boucles\").\n",
    "\n",
    "\n",
    "Les valeurs transitent via les poids de la couche d'entrée (X) à la couche cachée (l1), ces valeurs sont ensuite additionnées puis passent par une fonction sigmoïde.\n",
    "\n",
    "De la même manière, les valeurs obtenues dans la couche cachée (l1) passent à la couche de sortie (l2).\n",
    "\n",
    "L'étape de correction suit. La sortie (l2) est comparée aux valeurs attendues (y), puis passe par une dérivée de la sigmoïde. Nous obtenons l'importance de l'erreur sur la couche de sortie (l2_delta).\n",
    "\n",
    "Les poids permettent de calculer l'importance d'un neurone précédent sur l'erreur actuelle. Cette pondération suivie d'une dérivée de la sigmoïde permet de trouver l'importance de l'erreur sur la couche cachée (l1_delta).\n",
    "\n",
    "Nous modifions alors les deux couches de poids (syn0 et syn1) en fonction des erreurs trouvées.\n",
    "\n",
    "\n",
    "Le résultat est alors imprimé. D'abord le nombre d'itérations d'entrainement (la somme des boucles exécutées), puis le résultat du réseau \"entrainé\". La progression de l'entrainement est affichée également."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boucles = ???\n",
    "for j in range(boucles):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n",
    "    l2_delta = (y - l2)*(l2*(1-l2))\n",
    "    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += X.T.dot(l1_delta)\n",
    "    compteur += 1\n",
    "    accuracy.append(1 - np.mean([abs(x) for x in l2-y])) \n",
    "\n",
    "print(compteur)\n",
    "print(l2)\n",
    "plot = plt.figure()\n",
    "plt.plot(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La récurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux de neurones tels que présentés avant se basent sur une une donnée unique afin d'y apporter une réponse instantannée.\n",
    "Cependant, l'information doit souvent être traitée en prenant connaissance d'un contexte temporel. C'est notamment le cas de l'apprentissage sur du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant toute chose, les imports, comme précédemment. Il s'agit ici des modèles d'abstraction de Keras, très utiles plus simplifier la conception de réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer cela, nous allons apprendre à une machine comment écrire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant toute chose, nous devons charger le texte sur lequel la machine doit apprendre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/descartes.txt', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrairement aux valeurs numériques que le réseau a pu comprendre facilement dans la première partie, il est nécessaire de transformer notre modèle de données. Nous allons donc changer chaque lettre en un ensemble de 0 et de 1, le 1 de l'ensemble indiquant la position de la lettre dans l'alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la même occasion, nous mettons en place nos jeux d'entrainement et de validation. Le code peut sembler complexe au premier abord, mais il consiste juste à décaler le texte initial de 1 caractère vers la droite.\n",
    "Le jeu d'entrainement est donc le texte, tel quel, tandis que le jeu de validation est, pour tout caractère du texte, le caractère suivant. On cherche donc à deviner la prochaine lettre, sachant la lettre actuelle. Pour simplifier l'entrainement, on limite l'apprentissage à des chaines de taille SEQ_LENGTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "SEQ_LENGTH = ???\n",
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "X = np.zeros((len(data)//SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE))\n",
    "y = np.zeros((len(data)//SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE))\n",
    "for i in range(0, len(data)//SEQ_LENGTH):\n",
    "    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans la premièer partie, nous concevons notre réseau de neurones, à ceci près que la tâche est grandement simplifiée par Keras. Nous avons un modèle (Sequential), constitué de LAYER_DIM couches successives, chacune de taille HIDDEN_DIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = ???\n",
    "LAYER_NUM = ???\n",
    "boucles = 0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(SimpleRNN(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pouvoir comprendre le retour de la machine (un ensemble de nombres), nous définissons une méthode de traduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vient alors l'entrainement du modèle.\n",
    "Pour ne pas à avoir à relancer ce code, il boucle indéfiniment. Il affiche, toutes les STEPS boucles d'entrainement, la génération d'une chaine de caractère de longueur GENERATE_LENGTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_LENGTH = 100\n",
    "STEPS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(generate_text(model, GENERATE_LENGTH))\n",
    "while True:\n",
    "    boucles += STEPS\n",
    "    nb_epoch = 0\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=0, epochs=STEPS)\n",
    "    nb_epoch += STEPS\n",
    "    puts = generate_text(model, GENERATE_LENGTH)\n",
    "\n",
    "    clear_output()\n",
    "    print(boucles)\n",
    "    print(puts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà qui conclut la partie sur les réseaux de neurones. Il ne s'agit là que d'une présentation très basique qui reste limitée en comparaison des possibilités qu'offrent ces modèles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
